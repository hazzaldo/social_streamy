Project: Social Streamy (aka Social Streamy)
High-level

Each phase has: scope, exact tasks, files to touch, and a Definition of Done (DoD). Don’t allow the Agent to start the next phase until the DoD is met and demonstrated with logs/screens.

---

---

Phase 1 — Host ➜ Viewer (already drafted)

Goal: One host publishes, one viewer sees host video.
Files: replit.md, src/TestHarness.tsx, src/App.tsx, src/main.tsx, backend WS /ws.

Tasks (Agent)

Implement the minimal harness and route /harness.

Send joined_stream with userId (string).

Relay webrtc_offer|webrtc_answer|ice_candidate by toUserId.

Build WS URL from window.location (no localhost).

Health endpoint GET /\_version.

DoD (must show in logs)

Host: 🎥 Local tracks ready.

Host: 👤 Participant joined stream: <viewerId>.

Host: 📤 SENDING webrtc_offer ….

Viewer: 📥 RECEIVED webrtc_offer … → 📤 SENDING webrtc_answer ….

Host: ✅ Host setRemoteDescription(answer) ….

Viewer shows host video.

1. In Plan, paste:

Plan:
Add a minimal WebRTC test harness and acceptance tests.

Goals

One host publishes camera/mic.

One viewer joins and sees host video.

Signaling over WSS at /ws (no localhost in the browser).

Message shapes must exactly match:

server → host on viewer join: { type:'joined_stream', streamId, userId } (userId is a string)

relay: webrtc_offer|webrtc_answer|ice_candidate with { toUserId, fromUserId, … }

Tasks

Create a new file src/TestHarness.tsx containing a Host/Viewer page that:

Builds WS URL from window.location (https → wss).

Lets me pick Role = host/viewer.

Host: Connect WS → Start Host Media (await getUserMedia, add tracks before createOffer).

Viewer: Connect WS → Join as Viewer.

Logs clearly:

🎉 WS OPEN

🎥 Local tracks ready: …

👤 Participant joined stream: <viewerId> raw: {…}

📤 SENDING webrtc_offer …

📥 RECEIVED webrtc_offer …

📤 SENDING webrtc_answer …

✅ Host setRemoteDescription(answer) …

<video> tags use autoplay + playsInline (needed for iOS).

Add a route so I can open the page at /harness.

If the project already uses React Router: add <Route path="/harness" element={<TestHarness/>} /> to App.tsx, and ensure BrowserRouter wraps the app in main.tsx.

If no router, temporarily render <TestHarness/> directly in main.tsx (we’ll remove later).

Ensure backend sends joined_stream with userId (string) to the host when a viewer joins. Add a server log:

console.log('joined_stream -> host', { streamId, userId })

Confirm WS proxy path /ws is working on the repl domain and that large SDP frames are relayed intact.

Add a health endpoint GET /\_version returning { ts, git } so I can verify a fresh deploy.

Provide me the public URL for the repl (the one that opens from “Preview in new tab”).

Definition of Done (must show these logs)

Host tab: 🎥 Local tracks ready: …

Host tab: 👤 Participant joined stream: <viewerId>

Host tab: 📤 SENDING webrtc_offer …

Viewer tab: 📥 RECEIVED webrtc_offer …

Viewer tab: 📤 SENDING webrtc_answer …

Host tab: ✅ Host setRemoteDescription(answer) …

Viewer plays host video.

One host broadcasts live video/audio. Multiple viewers join and should immediately see the host’s feed. Monetization (coins → gifts / super messages) is separate and not needed for the MVP stream engine.

Roles

Host (creator): starts camera/mic, publishes tracks, sends WebRTC offers to each viewer.

Viewer: joins a stream, receives host tracks, sends answer back.

Signaling server: room membership + relay SDP/ICE over WebSocket (WSS).

Browser/Runtime Constraints

Must work on latest Chrome, Safari (iOS 16+), Edge.

Pages served over HTTPS; signaling over WSS (no ws://).

No hard-coded localhost or ports in browser code. Build WS URL from window.location.

Autoplay: video elements must include autoplay + playsInline.

Handle page reloads and reconnects idempotently.

WebRTC Architecture (required)

RTCPeerConnection created per viewer on host.

Host obtains local media before creating any offers.

ICE servers:

const pc = new RTCPeerConnection({
iceServers: [
{ urls: ['stun:stun.l.google.com:19302'] },
// TODO: replace with production TURN over TCP/TLS
// { urls:['turn:TURN_HOST:3478'], username:'u', credential:'p' }
]
})

Host adds tracks:

const stream = await navigator.mediaDevices.getUserMedia({ video: true, audio: true });
stream.getTracks().forEach(t => pc.addTrack(t, stream));

Viewers do not publish tracks (receive-only).

Signaling (WebSocket) Requirements
WS URL (client)
function wsUrl(path = '/ws') {
const { protocol, host } = window.location; // e.g. https://<repl>.replit.dev
const wsProto = protocol === 'https:' ? 'wss:' : 'ws:';
return `${wsProto}//${host}${path}`;
}

Message shapes (exact keys)

All IDs are strings. Use the same camelCase everywhere.

// client → server
{ type: 'join_stream', streamId: string, userId: string } // viewer joins
{ type: 'leave_stream', streamId: string, userId: string } // viewer leaves
{ type: 'webrtc_offer', toUserId: string, fromUserId: string, sdp: RTCSessionDescriptionInit }
{ type: 'webrtc_answer', toUserId: string, fromUserId: string, sdp: RTCSessionDescriptionInit }
{ type: 'ice_candidate', toUserId: string, fromUserId: string, candidate: RTCIceCandidateInit }

// server → host (when a viewer joins)
{ type: 'joined_stream', streamId: string, userId: string } // <- MUST include viewer userId

// server → client (relay)
{ type: 'webrtc_offer', fromUserId: string, sdp: RTCSessionDescriptionInit }
{ type: 'webrtc_answer', fromUserId: string, sdp: RTCSessionDescriptionInit }
{ type: 'ice_candidate', fromUserId: string, candidate: RTCIceCandidateInit }

// server → all in room
{ type: 'participant_count_update', streamId: string, count: number }

Non-negotiable: joined_stream must contain userId for the joining viewer. The host cannot target an offer without this.

Host detection (avoid false negatives)
const currentUserId = String(user?.id ?? '');
const streamOwnerId = String(streamData?.userId ?? '');
const isHost = currentUserId && streamOwnerId && currentUserId === streamOwnerId;

Required Client Flow
Host

Join room as host; fetch streamData.userId.

Get local media; add tracks; show self preview.

On {type:'joined_stream', userId:<viewerId>}:

Create RTCPeerConnection for that viewer.

Add local tracks to PC.

createOffer → setLocalDescription.

Send {type:'webrtc_offer', toUserId:<viewerId>, fromUserId:<hostId>, sdp}.

Viewer

Send {type:'join_stream', streamId, userId}.

On {type:'webrtc_offer'}:

setRemoteDescription(offer).

Create PC; set up ontrack to attach remote stream.

createAnswer → setLocalDescription.

Send {type:'webrtc_answer', toUserId:<hostId>, fromUserId:<viewerId>, sdp}.

Both

Relay ICE candidates with {type:'ice_candidate'} messages.

Connection State Management

Maintain a Map<viewerId, RTCPeerConnection> on host.

Clean up on leave_stream or iceconnectionstate === 'failed'|'disconnected' (with retry).

Heartbeat ping every 25s to avoid idle WS close (server can echo).

Acceptance Tests (Definition of Done)

Host join: console shows 🎥 Local tracks ready: >=1.

Viewer join: host console logs
👤 Participant joined stream: <viewerId> (value is not undefined).

Offer path: host logs 📤 SENDING webrtc_offer { toUserId: '<viewerId>' };
viewer logs 📥 RECEIVED webrtc_offer { fromUserId: '<hostId>' }.

Answer path: viewer logs 📤 SENDING webrtc_answer;
host logs 📥 RECEIVED webrtc_answer.

Playback: viewer video element fires loadedmetadata and plays (host’s feed visible).

Logging (required while debugging)

Host on join: 👑 HOST CHECK { currentUserId, streamOwnerId, isHost }

On joined_stream: log the raw payload and the resolved viewerId.

On each SDP send/receive: log sdpLen.

On WS connect: log URL actually used.

On ICE events: first and last candidate.

Known Pitfalls to Avoid

userId / userID casing mismatch → viewerId becomes undefined.

Comparing numeric vs string IDs for host detection.

Creating offers before tracks exist.

Using wss://localhost:undefined or any hard-coded host/port in browser.

Minimal Server Guarantees

Preserve query strings and large WS frames (SDPs may be big).

No compression that breaks WS frames.

Outbound UDP allowed for STUN; provide TURN over TCP/TLS if UDP is flaky.

Quick Health Endpoints

GET /\_version → { ts, git }

GET /health → { ok: true }

What to ask the Agent to do next (step-by-step)

Step 1 — Fix signaling + host detection

Ensure joined_stream includes userId (string).

In the client handler, read data.userId ?? data.userID ?? data.viewerId.

Normalize IDs to strings and compute isHost as shown above.

Add the required debug logs.

Step 2 — Correct WS URL

Replace any localhost fallbacks with the wsUrl(...) builder from window.location.

Step 3 — Enforce media-before-offer

In host flow, await getUserMedia, add tracks before createOffer.

Step 4 — Wire Offer/Answer/ICE (both sides)

Implement the exact message shapes and log sdpLen on send/receive.

Step 5 — Run Acceptance Tests

Use two devices (host: phone Safari/Chrome; viewer: desktop Chrome).

Verify logs match the “Definition of Done” section.

If any step fails, post the raw payloads + which log was missing.

You can paste these as discrete instructions to the Agent:

“Read replit.md, then patch the signaling so joined_stream includes userId and the client reads it safely (casing fallbacks). Add the logs shown.”

“Replace all WS URL constructors with the wsUrl helper derived from window.location.”

“Ensure host gets local media and adds tracks before creating any offers.”

“Implement and verify the offer → answer → ice flow with the exact message shapes. Print sdpLen on each hop.”

“Run the Acceptance Tests from replit.md and paste the console output for both host and viewer.”

Phase 1.1 — WS Reliability & TURN (infra hardening)

Goal: Make the same flow reliable on Mac + iPhone over Wi-Fi/Cell.

Tasks

Add STUN + TURN over TCP/TLS to ICE servers.

Add WS heartbeat every 25–30s; server echoes.

Log first/last ICE candidates, and ICE state transitions.

DoD

Same DoD as Phase 1, plus: iPhone Safari viewer works; no WS idle disconnects for 2 minutes.

---

---

Phase 2 — Add Guest (co-host) and Host↔Guest media

Goal: Host and Guest see/hear each other (bi-directional PC).

Files: Update src/TestHarness.tsx (role selector host/guest/viewer).
Messages (new):

cohost_request, cohost_invite, cohost_accept, cohost_ready.

Tasks

Implement signaling for co-host acceptance and emit cohost_ready to Host with guestUserId.

Host starts offer to Guest; both publish tracks.

On Host, show Guest preview (remote track from guest).

DoD

Host: 📤 SENDING webrtc_offer (to guest)… → ✅ setRemoteDescription(answer) (from guest).

Guest: 📥 RECEIVED webrtc_offer … → 📤 SENDING webrtc_answer ….

Both see each other’s video.

---

---

Phase 3 — Fan-out Guest to Viewers (two feeds to viewer)

Goal: Viewers see Host + Guest.

Tasks

On Host: when guestPcRef.ontrack fires, store guestStreamRef and add its tracks to every Viewer PC (existing and future).

Viewer ontrack: attach first video to Remote A, second to Remote B.

DoD

After Host+Guest are connected, a Viewer joins:

Host: 📤 SENDING webrtc_offer (to viewer)… → ✅ setRemoteDescription(answer).

Viewer: receives offer/answers; sees two videos (Host + Guest).

---

---

Phase 4 — Two co-host entry paths

Goal: Support both ways of becoming co-host.

Paths

In-room: viewer clicks “Request co-host” inside the room.

From Home: user sees all Live Streams as cards, and taps “Co-stream” on any live stream (not in room yet).

Tasks

Implement:

request_cohost (in-room) → server → host as cohost_request.

home_request_cohost (out-of-room) → server → host as cohost_request.

Host sends cohost_invite; viewer/guest sends cohost_accept; server emits cohost_ready to Host.

Add minimal UI buttons in harness for both paths.

DoD

Agent records a screen or pastes logs showing:

In-room flow succeeds to Phase-2 DoD.

From-Home flow succeeds to Phase-2 DoD.

---

---

Phase 5 — Game rails (no UI chrome yet)

What “game rails” means

Think of your live video as the transport (WebRTC), and the game as a set of state changes that both co-streamers (Host & Guest) must stay in sync on—start time, whose turn it is, current prompt, when the round ends, etc.
Phase 5 adds a tiny, predictable WebSocket protocol for those state changes. No design, no scoring, just the timing + turns + prompts signals.

Why you need this before building any actual game UI

Games like “Don’t Laugh”, “Role Roulette”, “Caption It” all share the same skeleton:

Init a round (which game, how long)

Start the round (the timer becomes authoritative)

Turns/Prompts go to one player at a time

End the round (time up or manual)

(Optional) Open vote for viewers and publish result

If Host & Guest don’t get the same start time / round id / prompts in lockstep, your game UIs will drift. Phase 5 prevents that drift.

What exactly happens in this phase (with context)

✅ Start/Stop round “truth”: The server becomes the source of truth for when a round is created and when the timer starts or ends. The Host sends “init” and “start”. The server then broadcasts those events so Host and Guest display the same timer.

✅ Turns/Prompts between the two players: The server can send a private prompt to only the active player (Host or Guest). The other player (and viewers) may get a generic or delayed version—your choice later. In Phase 5 we just show logs proving private delivery works.

✅ Viewers are read-only mirrors: They’re not playing; they just receive broadcast copies (e.g., “round started, 180s”). That lets their UIs show a timer/label, and later a vote prompt.

❌ No game chrome yet: We won’t build pretty overlays/timers—just add 2–4 buttons in the harness (Init/Start/End/Open Vote) and console logs to prove the flow.

❌ No scoring logic: We’re not deciding winners or computing laugh counts yet. (You can still optionally test vote open/result to prove the path exists.)

❌ No third player: Only Host & Guest interact. Viewers just watch (and can vote later). That keeps the protocol small and easy to verify.

Concrete example timeline (Don’t Laugh, 3 minutes)

Host clicks “Init Round”
Client → Server: {type:'round_init', game:'dont_laugh', durationSec:180}
Server → Host, Guest, Viewers: {type:'round_init_broadcast', roundId:'r1', durationSec:180}
➜ Everyone logs the same roundId (“we’re preparing r1”).

Host clicks “Start Round”
Client → Server: {type:'round_start', roundId:'r1'}
Server stamps time and broadcasts:
{type:'round_started', roundId:'r1', startedAt: <serverTs>, durationSec:180}
➜ Host & Guest start the same 180s timer; Viewers can show “Round live (3:00)”.

(Optional turns/prompts)
Server → Host only: {type:'turn_prompt', toUserId:'host', payload:{text:'Make them laugh without words'}}
Host performs; Guest reacts.
Guest sends a move: {type:'game_event', event:'reaction', payload:{smile:true}}
Server echoes to both players (and optionally viewers read-only):
{type:'game_event_broadcast', event:'reaction', ...}

Timer expires
Server → all: {type:'round_end', roundId:'r1', reason:'timeup'}
➜ Both players stop; viewers see the round end.

(Optional) open vote
Host (or server) triggers: {type:'vote_open', durationSec:10}
Viewers send {type:'vote_cast', vote:'host'|'guest'}
Server → all: {type:'vote_result', hostPct:62, guestPct:38}

What you’ll click in the harness (and why)

Init Round (Host): proves we can create a round and everyone hears about the same roundId.

Start Round (Host): proves the server is authoritative on the start time, so Host & Guest timers sync.

End Round (Host): proves server can stop a round for everyone, cleanly.

Open Vote (optional): proves the viewer → server → result broadcast loop.

How this unlocks the next steps

Once these rails work, any game UI can plug in:

Timers render from round_started.startedAt + durationSec

The “active player” sees turn_prompt

Inputs send game_event and render from game_event_broadcast

The end screen listens to round_end / vote_result

You won’t be debugging “why did my caption screen appear at different times?”—the rails already guarantee sync.

---

---

Phase 6 — Productionization

Goal: Make it resilient.

Tasks

Reconnect strategy for WS + ICE (disconnected → quick renegotiation; failed → rebuild PC).

Mute/switch camera via RTCRtpSender.replaceTrack().

Metrics logs: offer/answer sizes, time-to-first-frame, ICE completion time.

DoD

Simulate network blip; stream auto-recovers without manual refresh.

Copy-paste message for the Replit Plan box

Phased Delivery Plan (do not skip phases)

Phase 1 — Host ➜ Viewer
Implement the minimal /harness page and WS signaling per replit.md. Meet the DoD logs and viewer playback.

Phase 1.1 — Reliability & TURN
Add TURN over TCP/TLS and heartbeat; verify iPhone viewer works and WS stays alive.

Phase 2 — Add Guest (co-host)
Implement cohost_request/invite/accept/ready. Host↔Guest bi-directional PC; both see each other.

Phase 3 — Fan-out Guest to Viewers
Host forwards Guest tracks to all viewers so viewers see two videos.

Phase 4 — Two Entry Paths
Support in-room request_cohost and out-of-room home_request_cohost, both leading to cohost_ready.

Phase 5 — Game Rails
Wire round*\* and vote*\* events; verify broadcast to Host, Guest, Viewers.

Phase 6 — Productionization
Reconnect logic, replaceTrack, basic metrics.

Proceed one phase at a time. For each phase, paste the console logs/screenshot proving the Definition of Done before continuing.


---------------------
--------------------
Message shapes (authoritative server)

All IDs are strings. serverTs is server time (ms). Server is the single source of truth.

// Host tells server to prepare a round
{ "type":"round_init",  "streamId":"...", "hostId":"...", "game":"dont_laugh", "durationSec":180 }

// Server → Host & Guest (AND viewers as read-only)
{ "type":"round_init_broadcast", "streamId":"...", "game":"dont_laugh",
  "durationSec":180, "serverTs": 1760462000000, "roundId":"r-abc123" }

// Host tells server to start the round timer
{ "type":"round_start", "streamId":"...", "hostId":"...", "roundId":"r-abc123" }

// Server → Host & Guest (AND viewers read-only)
{ "type":"round_started", "streamId":"...", "roundId":"r-abc123",
  "startedAt":1760462010000, "durationSec":180 }

// Optional: server sends structured prompts/turns only to players
{ "type":"turn_prompt", "streamId":"...", "roundId":"r-abc123",
  "toUserId":"<host-or-guest>", "payload":{ "text":"Because it was Tuesday." } }

// Optional: a player sends a game action to server
{ "type":"game_event", "streamId":"...", "roundId":"r-abc123",
  "fromUserId":"...", "event":"answer_submitted", "payload":{ "text":"What day was it?" } }

// Server echoes sanitized events back to both players (and optionally viewers read-only)
{ "type":"game_event_broadcast", "streamId":"...", "roundId":"r-abc123",
  "fromUserId":"...", "event":"answer_submitted", "payload":{ "text":"..." },
  "serverTs":1760462023456 }

// End of round (time up or manual end by host)
{ "type":"round_end", "streamId":"...", "roundId":"r-abc123",
  "reason":"timeup" | "ended_by_host" }

// (Public mode only) voting window for viewers
{ "type":"vote_open", "streamId":"...", "roundId":"r-abc123", "durationSec":10 }
{ "type":"vote_cast", "streamId":"...", "roundId":"r-abc123",
  "viewerId":"...", "vote":"host" | "guest" }
{ "type":"vote_result", "streamId":"...", "roundId":"r-abc123",
  "hostPct":62, "guestPct":38, "total":123 }


Rules

Only Host may send round_init and round_start (Guest ignores if they try).

Server broadcasts round_init_broadcast and round_started to Host + Guest + Viewers so all UIs stay in sync.

Prompts/turns can be sent only to players (toUserId) so the guessing game works; viewers can optionally get a summarized or delayed version.

Harness changes (for this phase)

Add four buttons visible only when Role = Host:

Init Round → sends {type:'round_init', game, durationSec} (use a simple select for game and durationSec 180/300).

Start Round → sends {type:'round_start'}.

End Round → sends {type:'round_end'} (manual stop).

Open Vote (optional) → sends {type:'vote_open', durationSec:10}.

Add logs for all the server broadcasts above on Host, Guest, and Viewer.

Optional: Add one Guest button:

Submit Answer → sends {type:'game_event', event:'answer_submitted', payload:{text}} to show the echo path works.

What each role should receive

Host

Sees its own button clicks echoed by server broadcasts:

round_init_broadcast, round_started, game_event_broadcast, round_end

If vote is enabled: doesn’t cast votes, but sees vote_open and vote_result.

Guest

Receives the same broadcasts as Host.

May receive private prompts via turn_prompt (optional this phase).

May send game_event to server; sees the echoed game_event_broadcast.

Viewer

Receives read-only: round_init_broadcast, round_started, game_event_broadcast (if you choose), vote_open, vote_result, round_end.

Can cast a vote only when vote_open arrives (optional this phase).

Acceptance (Definition of Done)

Round start sync

Host clicks Init Round → Host, Guest, Viewer all log round_init_broadcast (same roundId).

Host clicks Start Round → Host, Guest, Viewer all log round_started (same roundId, startedAt, durationSec).

Events echo

Guest clicks Submit Answer → Host & Guest log game_event_broadcast with the same payload (viewers may log it read-only if you choose).

Round end

Either the server timer elapses or Host clicks End Round → Host, Guest, Viewer all log round_end (same roundId).

(Optional) Vote

Host clicks Open Vote → Viewers log vote_open.

Send a couple of {type:'vote_cast'} messages (you can simulate from console or harness).

All log vote_result.

If any one of those broadcasts is missing on Guest (or arrives with a different roundId), the phase isn’t done.

Minimal server pseudo (for Replit)
// On round_init (host-only)
state.rounds[streamId] = { id: newRoundId(), game, durationSec, startedAt: null };
broadcast(streamId, { type:'round_init_broadcast', streamId, game, durationSec, roundId: state.rounds[streamId].id, serverTs: Date.now() });

// On round_start (host-only)
const r = state.rounds[streamId]; r.startedAt = Date.now();
broadcast(streamId, { type:'round_started', streamId, roundId: r.id, startedAt: r.startedAt, durationSec: r.durationSec });

// On game_event (host or guest)
broadcastPlayers(streamId, { type:'game_event_broadcast', streamId, roundId: r.id, fromUserId, event, payload, serverTs: Date.now() });

// On round_end (manual or timer)
broadcast(streamId, { type:'round_end', streamId, roundId: r.id, reason });
clear state.rounds[streamId];

// Vote open/cast/result (optional)
on 'vote_open' → start 10s window, accept 'vote_cast' from viewers, then emit 'vote_result'.
