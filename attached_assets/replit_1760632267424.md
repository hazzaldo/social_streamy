Project: Social Streamy (aka Social Streamy)
High-level

1. In Plan, paste:

Plan:
Add a minimal WebRTC test harness and acceptance tests.

Goals

One host publishes camera/mic.

One viewer joins and sees host video.

Signaling over WSS at /ws (no localhost in the browser).

Message shapes must exactly match:

server → host on viewer join: { type:'joined_stream', streamId, userId } (userId is a string)

relay: webrtc_offer|webrtc_answer|ice_candidate with { toUserId, fromUserId, … }

Tasks

Create a new file src/TestHarness.tsx containing a Host/Viewer page that:

Builds WS URL from window.location (https → wss).

Lets me pick Role = host/viewer.

Host: Connect WS → Start Host Media (await getUserMedia, add tracks before createOffer).

Viewer: Connect WS → Join as Viewer.

Logs clearly:

🎉 WS OPEN

🎥 Local tracks ready: …

👤 Participant joined stream: <viewerId> raw: {…}

📤 SENDING webrtc_offer …

📥 RECEIVED webrtc_offer …

📤 SENDING webrtc_answer …

✅ Host setRemoteDescription(answer) …

<video> tags use autoplay + playsInline (needed for iOS).

Add a route so I can open the page at /harness.

If the project already uses React Router: add <Route path="/harness" element={<TestHarness/>} /> to App.tsx, and ensure BrowserRouter wraps the app in main.tsx.

If no router, temporarily render <TestHarness/> directly in main.tsx (we’ll remove later).

Ensure backend sends joined_stream with userId (string) to the host when a viewer joins. Add a server log:

console.log('joined_stream -> host', { streamId, userId })

Confirm WS proxy path /ws is working on the repl domain and that large SDP frames are relayed intact.

Add a health endpoint GET /\_version returning { ts, git } so I can verify a fresh deploy.

Provide me the public URL for the repl (the one that opens from “Preview in new tab”).

Definition of Done (must show these logs)

Host tab: 🎥 Local tracks ready: …

Host tab: 👤 Participant joined stream: <viewerId>

Host tab: 📤 SENDING webrtc_offer …

Viewer tab: 📥 RECEIVED webrtc_offer …

Viewer tab: 📤 SENDING webrtc_answer …

Host tab: ✅ Host setRemoteDescription(answer) …

Viewer plays host video.

One host broadcasts live video/audio. Multiple viewers join and should immediately see the host’s feed. Monetization (coins → gifts / super messages) is separate and not needed for the MVP stream engine.

Roles

Host (creator): starts camera/mic, publishes tracks, sends WebRTC offers to each viewer.

Viewer: joins a stream, receives host tracks, sends answer back.

Signaling server: room membership + relay SDP/ICE over WebSocket (WSS).

Browser/Runtime Constraints

Must work on latest Chrome, Safari (iOS 16+), Edge.

Pages served over HTTPS; signaling over WSS (no ws://).

No hard-coded localhost or ports in browser code. Build WS URL from window.location.

Autoplay: video elements must include autoplay + playsInline.

Handle page reloads and reconnects idempotently.

WebRTC Architecture (required)

RTCPeerConnection created per viewer on host.

Host obtains local media before creating any offers.

ICE servers:

const pc = new RTCPeerConnection({
iceServers: [
{ urls: ['stun:stun.l.google.com:19302'] },
// TODO: replace with production TURN over TCP/TLS
// { urls:['turn:TURN_HOST:3478'], username:'u', credential:'p' }
]
})

Host adds tracks:

const stream = await navigator.mediaDevices.getUserMedia({ video: true, audio: true });
stream.getTracks().forEach(t => pc.addTrack(t, stream));

Viewers do not publish tracks (receive-only).

Signaling (WebSocket) Requirements
WS URL (client)
function wsUrl(path = '/ws') {
const { protocol, host } = window.location; // e.g. https://<repl>.replit.dev
const wsProto = protocol === 'https:' ? 'wss:' : 'ws:';
return `${wsProto}//${host}${path}`;
}

Message shapes (exact keys)

All IDs are strings. Use the same camelCase everywhere.

// client → server
{ type: 'join_stream', streamId: string, userId: string } // viewer joins
{ type: 'leave_stream', streamId: string, userId: string } // viewer leaves
{ type: 'webrtc_offer', toUserId: string, fromUserId: string, sdp: RTCSessionDescriptionInit }
{ type: 'webrtc_answer', toUserId: string, fromUserId: string, sdp: RTCSessionDescriptionInit }
{ type: 'ice_candidate', toUserId: string, fromUserId: string, candidate: RTCIceCandidateInit }

// server → host (when a viewer joins)
{ type: 'joined_stream', streamId: string, userId: string } // <- MUST include viewer userId

// server → client (relay)
{ type: 'webrtc_offer', fromUserId: string, sdp: RTCSessionDescriptionInit }
{ type: 'webrtc_answer', fromUserId: string, sdp: RTCSessionDescriptionInit }
{ type: 'ice_candidate', fromUserId: string, candidate: RTCIceCandidateInit }

// server → all in room
{ type: 'participant_count_update', streamId: string, count: number }

Non-negotiable: joined_stream must contain userId for the joining viewer. The host cannot target an offer without this.

Host detection (avoid false negatives)
const currentUserId = String(user?.id ?? '');
const streamOwnerId = String(streamData?.userId ?? '');
const isHost = currentUserId && streamOwnerId && currentUserId === streamOwnerId;

Required Client Flow
Host

Join room as host; fetch streamData.userId.

Get local media; add tracks; show self preview.

On {type:'joined_stream', userId:<viewerId>}:

Create RTCPeerConnection for that viewer.

Add local tracks to PC.

createOffer → setLocalDescription.

Send {type:'webrtc_offer', toUserId:<viewerId>, fromUserId:<hostId>, sdp}.

Viewer

Send {type:'join_stream', streamId, userId}.

On {type:'webrtc_offer'}:

setRemoteDescription(offer).

Create PC; set up ontrack to attach remote stream.

createAnswer → setLocalDescription.

Send {type:'webrtc_answer', toUserId:<hostId>, fromUserId:<viewerId>, sdp}.

Both

Relay ICE candidates with {type:'ice_candidate'} messages.

Connection State Management

Maintain a Map<viewerId, RTCPeerConnection> on host.

Clean up on leave_stream or iceconnectionstate === 'failed'|'disconnected' (with retry).

Heartbeat ping every 25s to avoid idle WS close (server can echo).

Acceptance Tests (Definition of Done)

Host join: console shows 🎥 Local tracks ready: >=1.

Viewer join: host console logs
👤 Participant joined stream: <viewerId> (value is not undefined).

Offer path: host logs 📤 SENDING webrtc_offer { toUserId: '<viewerId>' };
viewer logs 📥 RECEIVED webrtc_offer { fromUserId: '<hostId>' }.

Answer path: viewer logs 📤 SENDING webrtc_answer;
host logs 📥 RECEIVED webrtc_answer.

Playback: viewer video element fires loadedmetadata and plays (host’s feed visible).

Logging (required while debugging)

Host on join: 👑 HOST CHECK { currentUserId, streamOwnerId, isHost }

On joined_stream: log the raw payload and the resolved viewerId.

On each SDP send/receive: log sdpLen.

On WS connect: log URL actually used.

On ICE events: first and last candidate.

Known Pitfalls to Avoid

userId / userID casing mismatch → viewerId becomes undefined.

Comparing numeric vs string IDs for host detection.

Creating offers before tracks exist.

Using wss://localhost:undefined or any hard-coded host/port in browser.

Minimal Server Guarantees

Preserve query strings and large WS frames (SDPs may be big).

No compression that breaks WS frames.

Outbound UDP allowed for STUN; provide TURN over TCP/TLS if UDP is flaky.

Quick Health Endpoints

GET /\_version → { ts, git }

GET /health → { ok: true }

What to ask the Agent to do next (step-by-step)

Step 1 — Fix signaling + host detection

Ensure joined_stream includes userId (string).

In the client handler, read data.userId ?? data.userID ?? data.viewerId.

Normalize IDs to strings and compute isHost as shown above.

Add the required debug logs.

Step 2 — Correct WS URL

Replace any localhost fallbacks with the wsUrl(...) builder from window.location.

Step 3 — Enforce media-before-offer

In host flow, await getUserMedia, add tracks before createOffer.

Step 4 — Wire Offer/Answer/ICE (both sides)

Implement the exact message shapes and log sdpLen on send/receive.

Step 5 — Run Acceptance Tests

Use two devices (host: phone Safari/Chrome; viewer: desktop Chrome).

Verify logs match the “Definition of Done” section.

If any step fails, post the raw payloads + which log was missing.

You can paste these as discrete instructions to the Agent:

“Read replit.md, then patch the signaling so joined_stream includes userId and the client reads it safely (casing fallbacks). Add the logs shown.”

“Replace all WS URL constructors with the wsUrl helper derived from window.location.”

“Ensure host gets local media and adds tracks before creating any offers.”

“Implement and verify the offer → answer → ice flow with the exact message shapes. Print sdpLen on each hop.”

“Run the Acceptance Tests from replit.md and paste the console output for both host and viewer.”
